{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.6.\n",
      "classifier.3.\n",
      "classifier.0.\n",
      "features.28.\n",
      "features.26.\n",
      "features.24.\n",
      "features.21.\n",
      "features.19.\n",
      "features.17.\n",
      "features.14.\n",
      "features.12.\n",
      "features.10.\n",
      "features.7.\n",
      "features.5.\n",
      "features.2.\n",
      "features.0.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "import pprint\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# def getModelLayers(model): is a function to return the layers of a model in a more clean and organized manner\n",
    "#                            that is efficient and easier for pruning purposes\n",
    "#\n",
    "#\n",
    "# The format of the returned data will be a LIST of TUPLES or as followed:\n",
    "#    cleanedLayers[i] represents the i_th layer (that needs to be pruned) counting from the output\n",
    "#         ---cleanedLayers[0] is the last layer of the model that needs to be pruned:\n",
    "#            -Therefore if the last layer is a convolutional layer, it will be stored...\n",
    "#            -However, if the last layer is a batch norm layer (doesn't need to be pruned) it will not store it.\n",
    "#            -If the last layer is batch norm and the second-to-last layer is a convolutional layer, cleanedLayers[0] will \n",
    "#            store the second-to-last layer and so forth.\n",
    "#\n",
    "#    At cleanedLayers[i] will be a tuple of the following data:\n",
    "#         ---cleanedLayers[i][0]: Name of the i_th to last layer to be pruned (string)\n",
    "#         ---cleanedLayers[i][1]: weights of the i_th to last layer, of type tensor\n",
    "#         ---cleanedLayers[i][2]: biases of the i_th to last layer, of type tensor\n",
    "#\n",
    "# NOTES:\n",
    "#         ---memory is shared between the model that is passed into getModelLayers and the returned list,\n",
    "#            therefore any changed values are permanately changed unless a deep copy is made\n",
    "\n",
    "def getModelLayers(model):\n",
    "    \n",
    "    #load state dictionary\n",
    "    sdict = model.state_dict()\n",
    "    \n",
    "    \n",
    "    #store\n",
    "    #modelLayers = [(layer, param) for (layer, param) in sdict.items()]\n",
    "    \n",
    "    \n",
    "    #dictionary of layers with their (names, weight/bias) --- adding only layers we must prune\n",
    "    pruningWeights = dict()\n",
    "    pruningBiases = dict()\n",
    "    \n",
    "    #for i in range(len(modelLayers)):\n",
    "    #    if (not 'bn' in modelLayers[i][0] and not 'downsample' in modelLayers[i][0] and not 'pool' in modelLayers[i][0]):\n",
    "    #        if 'weight' in modelLayers[i][0]:\n",
    "    #            #print(modelLayers[i][0][:-len('weight')])\n",
    "    #            pruningWeights[modelLayers[i][0]] = (modelLayers[i][1])\n",
    "    #        elif('bias' in modelLayers[i][0]):\n",
    "    #            pruningBiases[modelLayers[i][0]] = (modelLayers[i][1])\n",
    "\n",
    "    for (layer_name, params) in sdict.items():\n",
    "        if (not 'bn' in layer_name and not 'downsample' in layer_name and not 'pool' in layer_name):\n",
    "            if 'weight' in layer_name:\n",
    "                pruningWeights[layer_name] = (params)\n",
    "            elif('bias' in modelLayers[i][0]):\n",
    "                pruningBiases[layer_name] = (params)\n",
    "    \n",
    "    cleanedLayers = []    \n",
    "    for weightName, weights in pruningWeights.items():\n",
    "        \n",
    "        #if layer as biases append weights + biases\n",
    "        if weightName[:-len('weight')]+'bias' in pruningBiases:\n",
    "            cleanedLayers.append((weightName[:-len('weight')], weights, pruningBiases[weightName[:-len('weight')]+'bias']))\n",
    "        \n",
    "        #if layer doesn't have biases, append weights and an empty tensor for biases\n",
    "        else:\n",
    "            cleanedLayers.append((weightName[:-len('weight')], weights, torch.FloatTensor([])))\n",
    "    \n",
    "    #reverse the list so cleanedLayers[0] is the last layer of the network -- aka the layer closest to output\n",
    "    cleanedLayers.reverse()\n",
    "    return (cleanedLayers)\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "#Lets use vgg16 to because it is small\n",
    "model = models.vgg16()\n",
    "\n",
    "modelLayers = getModelLayers(model)\n",
    "\n",
    "#An example of how to iterate through modelLayers\n",
    "#only printing names of the layer\n",
    "for name, weights, biases in modelLayers:\n",
    "        print(name)#, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.6.\n",
      "classifier.3.\n",
      "classifier.0.\n",
      "features.28.\n",
      "features.26.\n",
      "features.24.\n",
      "features.21.\n",
      "features.19.\n",
      "features.17.\n",
      "features.14.\n",
      "features.12.\n",
      "features.10.\n",
      "features.7.\n",
      "features.5.\n",
      "features.2.\n",
      "features.0.\n"
     ]
    }
   ],
   "source": [
    "#Here is another way\n",
    "for i in range(len(modelLayers)):\n",
    "    name, weights, biases = modelLayers[i]          \n",
    "    print(name)#, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 classifier.6.\n",
      "1 classifier.3.\n",
      "2 classifier.0.\n",
      "3 features.28.\n",
      "4 features.26.\n",
      "5 features.24.\n",
      "6 features.21.\n",
      "7 features.19.\n",
      "8 features.17.\n",
      "9 features.14.\n",
      "10 features.12.\n",
      "11 features.10.\n",
      "12 features.7.\n",
      "13 features.5.\n",
      "14 features.2.\n",
      "15 features.0.\n"
     ]
    }
   ],
   "source": [
    "#and another...\n",
    "for i, (name, weights, biases) in enumerate(modelLayers):\n",
    "    print(i, name)#, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers: 16\n"
     ]
    }
   ],
   "source": [
    "#lastly, lets check this across multiple models and see if it still works\n",
    "\n",
    "#for vgg16\n",
    "print('number of layers: {}'.format(len(modelLayers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers: 50\n"
     ]
    }
   ],
   "source": [
    "#for resnet50\n",
    "print('number of layers: {}'.format(len(getModelLayers(models.resnet50()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers: 101\n"
     ]
    }
   ],
   "source": [
    "#for resnet101\n",
    "print('number of layers: {}'.format(len(getModelLayers(models.resnet101()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers: 152\n"
     ]
    }
   ],
   "source": [
    "#for resnet152\n",
    "print('number of layers: {}'.format(len(getModelLayers(models.resnet152()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers: 8\n"
     ]
    }
   ],
   "source": [
    "#for alexnet\n",
    "print('number of layers: {}'.format(len(getModelLayers(models.alexnet()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers: 89\n"
     ]
    }
   ],
   "source": [
    "#for inception\n",
    "print('number of layers: {}'.format(len(getModelLayers(models.inception_v3()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc.\n",
      "Mixed_7c.branch3x3dbl_3b.conv.\n",
      "Mixed_7c.branch3x3dbl_3a.conv.\n",
      "Mixed_7c.branch3x3dbl_2.conv.\n",
      "Mixed_7c.branch3x3dbl_1.conv.\n",
      "Mixed_7c.branch3x3_2b.conv.\n",
      "Mixed_7c.branch3x3_2a.conv.\n",
      "Mixed_7c.branch3x3_1.conv.\n",
      "Mixed_7c.branch1x1.conv.\n",
      "Mixed_7b.branch3x3dbl_3b.conv.\n",
      "Mixed_7b.branch3x3dbl_3a.conv.\n",
      "Mixed_7b.branch3x3dbl_2.conv.\n",
      "Mixed_7b.branch3x3dbl_1.conv.\n",
      "Mixed_7b.branch3x3_2b.conv.\n",
      "Mixed_7b.branch3x3_2a.conv.\n",
      "Mixed_7b.branch3x3_1.conv.\n",
      "Mixed_7b.branch1x1.conv.\n",
      "Mixed_7a.branch7x7x3_4.conv.\n",
      "Mixed_7a.branch7x7x3_3.conv.\n",
      "Mixed_7a.branch7x7x3_2.conv.\n",
      "Mixed_7a.branch7x7x3_1.conv.\n",
      "Mixed_7a.branch3x3_2.conv.\n",
      "Mixed_7a.branch3x3_1.conv.\n",
      "AuxLogits.fc.\n",
      "AuxLogits.conv1.conv.\n",
      "AuxLogits.conv0.conv.\n",
      "Mixed_6e.branch7x7dbl_5.conv.\n",
      "Mixed_6e.branch7x7dbl_4.conv.\n",
      "Mixed_6e.branch7x7dbl_3.conv.\n",
      "Mixed_6e.branch7x7dbl_2.conv.\n",
      "Mixed_6e.branch7x7dbl_1.conv.\n",
      "Mixed_6e.branch7x7_3.conv.\n",
      "Mixed_6e.branch7x7_2.conv.\n",
      "Mixed_6e.branch7x7_1.conv.\n",
      "Mixed_6e.branch1x1.conv.\n",
      "Mixed_6d.branch7x7dbl_5.conv.\n",
      "Mixed_6d.branch7x7dbl_4.conv.\n",
      "Mixed_6d.branch7x7dbl_3.conv.\n",
      "Mixed_6d.branch7x7dbl_2.conv.\n",
      "Mixed_6d.branch7x7dbl_1.conv.\n",
      "Mixed_6d.branch7x7_3.conv.\n",
      "Mixed_6d.branch7x7_2.conv.\n",
      "Mixed_6d.branch7x7_1.conv.\n",
      "Mixed_6d.branch1x1.conv.\n",
      "Mixed_6c.branch7x7dbl_5.conv.\n",
      "Mixed_6c.branch7x7dbl_4.conv.\n",
      "Mixed_6c.branch7x7dbl_3.conv.\n",
      "Mixed_6c.branch7x7dbl_2.conv.\n",
      "Mixed_6c.branch7x7dbl_1.conv.\n",
      "Mixed_6c.branch7x7_3.conv.\n",
      "Mixed_6c.branch7x7_2.conv.\n",
      "Mixed_6c.branch7x7_1.conv.\n",
      "Mixed_6c.branch1x1.conv.\n",
      "Mixed_6b.branch7x7dbl_5.conv.\n",
      "Mixed_6b.branch7x7dbl_4.conv.\n",
      "Mixed_6b.branch7x7dbl_3.conv.\n",
      "Mixed_6b.branch7x7dbl_2.conv.\n",
      "Mixed_6b.branch7x7dbl_1.conv.\n",
      "Mixed_6b.branch7x7_3.conv.\n",
      "Mixed_6b.branch7x7_2.conv.\n",
      "Mixed_6b.branch7x7_1.conv.\n",
      "Mixed_6b.branch1x1.conv.\n",
      "Mixed_6a.branch3x3dbl_3.conv.\n",
      "Mixed_6a.branch3x3dbl_2.conv.\n",
      "Mixed_6a.branch3x3dbl_1.conv.\n",
      "Mixed_6a.branch3x3.conv.\n",
      "Mixed_5d.branch3x3dbl_3.conv.\n",
      "Mixed_5d.branch3x3dbl_2.conv.\n",
      "Mixed_5d.branch3x3dbl_1.conv.\n",
      "Mixed_5d.branch5x5_2.conv.\n",
      "Mixed_5d.branch5x5_1.conv.\n",
      "Mixed_5d.branch1x1.conv.\n",
      "Mixed_5c.branch3x3dbl_3.conv.\n",
      "Mixed_5c.branch3x3dbl_2.conv.\n",
      "Mixed_5c.branch3x3dbl_1.conv.\n",
      "Mixed_5c.branch5x5_2.conv.\n",
      "Mixed_5c.branch5x5_1.conv.\n",
      "Mixed_5c.branch1x1.conv.\n",
      "Mixed_5b.branch3x3dbl_3.conv.\n",
      "Mixed_5b.branch3x3dbl_2.conv.\n",
      "Mixed_5b.branch3x3dbl_1.conv.\n",
      "Mixed_5b.branch5x5_2.conv.\n",
      "Mixed_5b.branch5x5_1.conv.\n",
      "Mixed_5b.branch1x1.conv.\n",
      "Conv2d_4a_3x3.conv.\n",
      "Conv2d_3b_1x1.conv.\n",
      "Conv2d_2b_3x3.conv.\n",
      "Conv2d_2a_3x3.conv.\n",
      "Conv2d_1a_3x3.conv.\n"
     ]
    }
   ],
   "source": [
    "#I think this is wrong, for the sake of showing that my function probably doesn't work\n",
    "#across all models, lets take a look\n",
    "modelLayers = getModelLayers(models.inception_v3())\n",
    "for name, _, _ in modelLayers:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These layers all look like valid layers but I am fairly certain that they aren't.\n",
    "#      --- Some of these layers shouldn't be pruned\n",
    "\n",
    "#I'm pointing this out because we have to remember not to blindly use this function on any model architecture.\n",
    "\n",
    "#For our purposes it should work as we are only using resnet101, alexnet, and vgg16 and \n",
    "#      --- This function works for them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
